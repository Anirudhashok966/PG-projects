{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atfnxOOtsVKR"
   },
   "source": [
    "# Definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) What is Reinforcement Learning?\n",
    "\n",
    "ans) Reinforcement learning involve the series of techniques where an agent (usually a machine), interacts with the environment  and gets a feedback. This feedback is the reward or a penatly for the chosen action along with the new state. The goal of the agent is to maximize rewards. The idea is to find out an optimal path or approach, through random actions, ( still part of the policy of the agent ), that ultimately maximizes the reward. \n",
    "\n",
    "2) Explain key terms:\n",
    "\n",
    "2.a) Environment: Enviroment is the space at which the agent takes actions in. The environment grants the agent rewards or penalties for the action chosen along with the new state for the agent. If the new state granted for the agent is a terminating state, the algorithm ends.\n",
    "\n",
    "2.b) State: State is the possible location an agent can start, end, or land at after an action. States, depending on what nature they are, can yield rewards or penalties. If the state is terminating, that is the end goal, then it rewards for the agent for completing the task. Else, states can penalize the agent, for loitering and increasing the run time. States also provide the information on the environment.\n",
    "\n",
    "2.c) Action: Action is the movement that an agent can make. The possible movements is often declared in the policy, where the policy is the behaviour of the agent. For example, a taxi in the state-space, that is, all possible states, can move, north, south, east and west and this rule is declared in the policy. So action is the way an agent can move or transition across states.\n",
    "\n",
    "2.d)Reward: Rewards or penalties are the necessary points, either in addition or deduction, for keeping the agent continue actions, to ultimatley find out the optimal path. Rewards are granted for correct actions/ good behaviour and penalties for the ones that are undesired.\n",
    "\n",
    "2.e) Agent: The agent is the learner in a reinforcement problem. The agent takes decisions and learns through it, ultimatley achieving its set objective. The agent interacts with its environment and it is granted rewards/penalties and new states till it achieves its end goal. The agent's behaviour is declared in the policy and with it transitions across states to reach the end goal.\n",
    "\n",
    "2.f) Greedy Algorithm: A greedy algorithm is one that maximizes its rewards. Such algorithm does not venture or explore to find out other possibilities or paths, and is usually restricted to one optimal path it finds first. An example would be the Q-learning algorithm, where the agent tries to maximize its Q values. The algorithm is fast at the cost of possibilities it would find if it were to follow an explorative algorithm.\n",
    "\n",
    "2.e) Epsilon Greedy Algorithm: To solve the problem of a greedy algorithm, a random value is generated. If this random value is less than epsilon, it would follow an explorative algoritm such as a SARSA approch. If not, the algorithm sticks to being greedy. Such an algorithm helps to strike a balance between being greedy and exploitative, yielding the benefits of both. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mouse-Cheese-Cat Maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a state-action-reward-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4BpkA_P5sVKS"
   },
   "outputs": [],
   "source": [
    "R = pd.read_csv('mouse.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJ8kwrWysVKT",
    "outputId": "5707bb94-c524-4ec9-a277-2a72e7edef32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[   0,    0,    0,    0],\n",
       "        [   0, -100,    0, -100],\n",
       "        [   0,    0,    0,    0],\n",
       "        [   0, -100,    0,  100]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = np.matrix(R)\n",
    "R\n",
    "#zero for every empty state. 100 for the state with cheese and -100 for the state with a cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7lW0g3GDsVKU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Qmatrix\n",
    "\n",
    "Q = np.zeros([4,4]);Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize gamma and initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "kVH7HMuXsVKU"
   },
   "outputs": [],
   "source": [
    "gamma = 0.4\n",
    "initial_state = 2 # initial state 2 works and gamma 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available actions function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "I-CydptqsVKV"
   },
   "outputs": [],
   "source": [
    "def avl_actions(state):\n",
    "    current_state_row = R[state,]\n",
    "    av_act = np.where(current_state_row > -1)[1]\n",
    "    return av_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "VmHFUb0lsVKV"
   },
   "outputs": [],
   "source": [
    "#available actions for current state = 2\n",
    "\n",
    "available_act = avl_actions(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ju-57Az-sVKV",
    "outputId": "38c4c509-4407-4fe0-f3cc-04712966e1a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random action within policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "s-Jr2h5-sVKV"
   },
   "outputs": [],
   "source": [
    "#random action performed in the range of all possible actions actions\n",
    "\n",
    "def sample_next_action(available_actions_range):\n",
    "    next_action = int(np.random.choice(available_actions_range,1))\n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "al7C-Kg9sVKW"
   },
   "outputs": [],
   "source": [
    "action = sample_next_action(available_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AK4hyq9zsVKW",
    "outputId": "2937ead0-195d-4f1f-a99a-d18e28c454f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the Q-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "rfgRmgqSsVKW"
   },
   "outputs": [],
   "source": [
    "#update Q matrix based on the path taken\n",
    "\n",
    "def update(current_state,action,gamma):\n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[0]\n",
    "\n",
    "    if max_index.shape[0] >1:\n",
    "        max_index = int(np.random.choice(max_index, size=1))\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "    max_value = Q[action,max_index]\n",
    "    Q[current_state,action] = R[current_state,action] + gamma*max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "SySKjd1hsVKX"
   },
   "outputs": [],
   "source": [
    "\n",
    "update(initial_state,action,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vDmd7yRsVKX",
    "outputId": "1894b7a0-8c46-415f-d3e5-8a047fe50c93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "VU2swsweyGlw"
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1500):\n",
    "    current_state = np.random.randint(0,int(Q.shape[0]))\n",
    "    available_act = avl_actions(current_state)\n",
    "    action = sample_next_action(available_act)\n",
    "    update(current_state,action,gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Q matrix (trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ts6IEF0i2v8m",
    "outputId": "b605934d-3094-4ae1-fee1-1c5a69ad7c0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Matrix - Trained\n",
      "[[ 16.    6.4  16.   40. ]\n",
      " [ 16.    0.   16.    0. ]\n",
      " [ 16.    6.4  16.   40. ]\n",
      " [ 16.    0.   16.  100. ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q Matrix - Trained\")\n",
    "print((Q / np.max(Q)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLTNzEWV3d5I",
    "outputId": "5996603b-a92e-429a-8d0a-da0854a28935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected path:\n",
      "[1, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "current_state =1 #change it accordingly, to get different results \n",
    "steps = [current_state]\n",
    "\n",
    "while current_state != 3:\n",
    "    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[0]\n",
    "\n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index, size=1))\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "\n",
    "    steps.append(next_step_index)\n",
    "    current_state = next_step_index\n",
    "\n",
    "print(\"The selected path:\")\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
